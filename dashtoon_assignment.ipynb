{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Dataset - https://www.kaggle.com/datasets/ikarus777/best-artworks-of-all-time****"
      ],
      "metadata": {
        "id": "2S0zsIcVV3Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "DmO2rFrhUD-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zojAMlf1Q9dU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SCet285iWNlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Content and Style Layers**"
      ],
      "metadata": {
        "id": "bog65_V9UPIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_layers = ['block4_conv2']\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n"
      ],
      "metadata": {
        "id": "6hJFQ_-KRB90"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Content and Style Layers**"
      ],
      "metadata": {
        "id": "7fU3ZAa4Ufoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    image_paths = [os.path.join(dataset_path, filename) for filename in os.listdir(dataset_path)]\n",
        "    images = [load_image(path) for path in image_paths]\n",
        "    return images\n",
        "\n",
        "dataset_path = \"path/to/dataset\"\n",
        "content_images = load_dataset(os.path.join(dataset_path, \"content\"))\n",
        "style_images = load_dataset(os.path.join(dataset_path, \"style\"))\n"
      ],
      "metadata": {
        "id": "OlTpdkjLUiVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the Model**"
      ],
      "metadata": {
        "id": "BgClT9bEUm6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(input_shape=(224, 224, 3), num_content_layers=1, num_style_layers=5):\n",
        "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    content_outputs = [base_model.get_layer(f'block{idx}_conv2').output for idx in range(1, num_content_layers + 1)]\n",
        "    style_outputs = [base_model.get_layer(f'block{idx}_conv1').output for idx in range(1, num_style_layers + 1)]\n",
        "\n",
        "    model_outputs = content_outputs + style_outputs\n",
        "\n",
        "    return Model(inputs=base_model.input, outputs=model_outputs)\n",
        "\n",
        "# Modify the number of content and style layers based on your requirements\n",
        "num_content_layers = 1\n",
        "num_style_layers = 5\n",
        "\n",
        "model = get_model(num_content_layers=num_content_layers, num_style_layers=num_style_layers)\n",
        "model.trainable = False\n"
      ],
      "metadata": {
        "id": "iK86qdcMSUVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Loss Functions**"
      ],
      "metadata": {
        "id": "8Brd6_6MUruu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(base_content, target):\n",
        "    # Assuming both base_content and target are 3D tensors (height, width, channels)\n",
        "    return tf.reduce_mean(tf.square(base_content - target))\n",
        "\n",
        "def gram_matrix(x):\n",
        "    # Assuming x is a 3D tensor (height, width, channels)\n",
        "    channels = int(x.shape[-1])\n",
        "    a = tf.reshape(x, (-1, channels))\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def style_loss(base_style, gram_target):\n",
        "    # Assuming base_style is a 3D tensor (height, width, channels)\n",
        "    return tf.reduce_mean(tf.square(gram_matrix(base_style) - gram_target))\n"
      ],
      "metadata": {
        "id": "ARBJAQidSlKu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Total Variation Loss**"
      ],
      "metadata": {
        "id": "KDqlTBX8UvsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_variation_loss(img, tv_weight=1e-6):\n",
        "    return tv_weight * tf.reduce_sum(tf.image.total_variation(img))\n"
      ],
      "metadata": {
        "id": "ualgY3JxS-B1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate the Model Input**"
      ],
      "metadata": {
        "id": "IMB0cCA_U3Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_generated_images = len(content_images)\n",
        "\n",
        "generated_images = [\n",
        "    tf.Variable(content_image, dtype=tf.float32)\n",
        "    for _ in range(num_generated_images)\n",
        "]\n"
      ],
      "metadata": {
        "id": "3w_CAnsxU1Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Optimizer**"
      ],
      "metadata": {
        "id": "_L7gVV6lU7f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam(\n",
        "    learning_rate=0.001,  # You may need to adjust this based on your dataset and model\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-08,\n",
        ")\n"
      ],
      "metadata": {
        "id": "opymzGb2THI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "BOi8w6c5U_CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, generated_image, content_target, style_targets, tv_weight=1e-6):\n",
        "    model_outputs = model(generated_image)\n",
        "    content_outputs = model_outputs[:len(content_layers)]\n",
        "    style_outputs = model_outputs[len(content_layers):]\n",
        "\n",
        "    content_loss_value = 0\n",
        "    style_loss_value = 0\n",
        "\n",
        "    for target_content, generated_content in zip(content_target, content_outputs):\n",
        "        content_loss_value += content_loss(generated_content[0], target_content)\n",
        "\n",
        "    for target_style, generated_style in zip(style_targets, style_outputs):\n",
        "        style_loss_value += style_loss(generated_style[0], target_style)\n",
        "\n",
        "    tv_loss_value = total_variation_loss(generated_image, tv_weight=tv_weight)\n",
        "\n",
        "    total_loss = content_loss_value + style_loss_value + tv_loss_value\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "lyiheyHwTmE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "UiiSKNuCVC8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(model, image, content_target, style_targets)\n",
        "        loss += total_variation_weight * total_variation_loss(image)\n",
        "\n",
        "    grads = tape.gradient(loss, image)\n",
        "    optimizer.apply_gradients([(grads, image)])\n",
        "\n",
        "    # Clip the image values to be within the valid range [0.0, 1.0]\n",
        "    image.assign(tf.clip_by_value(image, 0.0, 1.0))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for generated_image in generated_images:\n",
        "        train_step(generated_image)\n",
        "\n",
        "# Access the generated images as NumPy arrays\n",
        "final_images = [generated_image.numpy().squeeze() for generated_image in generated_images]\n"
      ],
      "metadata": {
        "id": "lw-u5xgwTy7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Style Adaptation**"
      ],
      "metadata": {
        "id": "-WFUUDlpWfO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_styles(style1, style2, alpha):\n",
        "    # Interpolate style features between two styles\n",
        "    interpolated_style = alpha * style1 + (1 - alpha) * style2\n",
        "    return interpolated_style\n",
        "\n",
        "\n",
        "style1 = model.predict(style_image1)\n",
        "style2 = model.predict(style_image2)\n",
        "\n",
        "alpha = 0.5  # Adjust alpha for the desired interpolation level\n",
        "new_style = interpolate_styles(style1, style2, alpha)\n",
        "\n"
      ],
      "metadata": {
        "id": "29ccQeVtVkip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n"
      ],
      "metadata": {
        "id": "rrMVbn_rWXvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_image(image_path, target_size=(224, 224)):\n",
        "    img = image.load_img(image_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array)\n",
        "\n",
        "\n",
        "content_image = load_image(\"content.jpg\")\n",
        "style_image = load_image(\"style.jpg\")\n",
        "style transfer function\n",
        "def style_transfer(content_image, style_image, model, num_epochs=10):\n",
        "    generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Measure the time taken for style transfer\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "content_target = model.predict(content_image)\n",
        "style_targets = model.predict(style_image)\n",
        "\n",
        "# Perform style transfer\n",
        "generated_image = style_transfer(content_image, style_image, model, num_epochs=10)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Style transfer took {elapsed_time} seconds.\")\n"
      ],
      "metadata": {
        "id": "uZfu9KvvVn8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}